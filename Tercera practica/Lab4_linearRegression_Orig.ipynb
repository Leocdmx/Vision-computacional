{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bQyBOaj8pUp"
   },
   "source": [
    "# Assignment: Linear Regression\n",
    "In this assignment you will implement Linear Regression for a very simple test case. Please fill into the marked places of the code\n",
    "  \n",
    "  (1) the cost function\n",
    "  (2) the update function for Gradient Descent\n",
    "\n",
    "Pieces of code that need to be updated are marked with \"HERE YOU ...\"\n",
    "\n",
    "This assignment is kept very simple on purpose to help you familiarize with Linear Regression and its implementation using python (Jupyter notebooks). Feel free to make some useful tests such as, but not limited to:\n",
    "- What happens if the learning rate is too high or too low?\n",
    "- Can Linear Regression really find the absolute global minimum?\n",
    "- What effect does it have if you change the initial guess for the gradient descent to something completely off?\n",
    "- What happens if you are not updating thet0 and thet1 \"simultaneously\" but you are updating both parameters in separate for loops (see below)?\n",
    "- You can try to turn this code for Linear Regression into an implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GibImbUt9M4X"
   },
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJqENzQL6_h7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dh6VL5uH9TB8"
   },
   "source": [
    "## Function to calculate the Gradiend descent\n",
    "You may find helpful the use of cost (one of the costFunction output parameters) to debug this method\n",
    "Hint: print(\"Iteration %d | Cost: %f\" % (i, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TDaV-br9agU"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, m, maxsteps):\n",
    "    # HERE YOU HAVE TO IMPLEMENT THE UPDATE OF THE PARAMETERS\n",
    "    thetaHist=np.empty([maxsteps, 2])\n",
    "    xTrans = x.transpose()\n",
    "    for i in range(0, maxsteps):\n",
    "        #theta = theta - ???\n",
    "        thetaHist[i] = theta\n",
    "   \n",
    "    return theta, thetaHist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uuU6OmGcAEFn"
   },
   "source": [
    "## Function to calcultate the cost function\n",
    "The cost function template is returning two parameters, loss and cost. We proposed these two paremeters to facilitate the implementation having not only the cost but also the difference between y and the prediction directly (loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqXN74O2AHw3"
   },
   "outputs": [],
   "source": [
    "def costFunction(x, y, theta):\n",
    "    # HERE YOU HAVE TO IMPLEMENT THE COST FUNCTION\n",
    "    return cost, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjmlKvgS9fnl"
   },
   "source": [
    "## Define some training data\n",
    "To test your algorithm it is a good idea to start with very simple test data where you know the right answer. So let's put all data points on a line first. Variables x and y represent a (very simple) training set (a dataset with 9 instances). Feel free to play with this test data or use a more realistic one.\n",
    "\n",
    "$NOTE:$ The column with 1â€™s included in the variable x is used to facilitate the calculations in the Gradient Descent function (do you remember the x<sub>0</sub> to use the matrix form? If not, revise the lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3TVU7If9jfU"
   },
   "outputs": [],
   "source": [
    "x=np.array([[1, 0], [1, 0.5], [1, 1], [1, 1.5], [1, 2], [1, 2.5], [1, 3], [1, 4], [1, 5]])\n",
    "y=np.array([0, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8SNNRRf9mad"
   },
   "source": [
    "## Calculate length of training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QEvl3FMx9ujU"
   },
   "outputs": [],
   "source": [
    "m, n = np.shape(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NfTCyK449w4s"
   },
   "source": [
    "## Plot training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9FP6f_l79y7-"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1)  # An empty figure with no axes\n",
    "plt.plot(x[:,1], y, 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHEXm5GT9118"
   },
   "source": [
    "## Cost function\n",
    "Also it is useful for simple test cases to not just run an optimization but first to do a systematic search. So let us first calculate the values of the cost function for different parameters theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qQTbR1G-BQ9"
   },
   "outputs": [],
   "source": [
    "theta0 = np.arange(-2, 2.01, 0.25)\n",
    "theta1 = np.arange(-2, 3.01, 0.25)\n",
    "\n",
    "# Calculate values of the cost function\n",
    "for i in range(0, len(theta0)):  \n",
    "    for j in range(0, len(theta1)):\n",
    "        # HERE YOU HAVE TO ADD THE COST FUNCTION FROM THE LECTURE\n",
    "        # J(i,j) = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRWnwwK2-M5c"
   },
   "source": [
    "## Visualize the cost function\n",
    "Let us do some test plots to see the cost function J and to analyze how it depends on the parameters theta0 and theta1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSVXIfYp-Tnz"
   },
   "outputs": [],
   "source": [
    "theta0, theta1 = np.meshgrid(theta0, theta1)\n",
    "fig2 = plt.figure(2)\n",
    "ax = fig2.add_subplot(121, projection=\"3d\")\n",
    "surf = ax.plot_surface(theta0, theta1, np.transpose(J))\n",
    "ax.set_xlabel('theta 0')\n",
    "ax.set_ylabel('theta 1')\n",
    "ax.set_zlabel('Cost J')\n",
    "ax.set_title('Cost function Surface plot')\n",
    "\n",
    "ax = fig2.add_subplot(122)\n",
    "contour = ax.contour(theta0, theta1, np.transpose(J))\n",
    "ax.set_xlabel('theta 0')\n",
    "ax.set_ylabel('theta 1')\n",
    "ax.set_title('Cost function Contour plot')\n",
    "\n",
    "fig2.subplots_adjust(bottom=0.1, right=1.5, top=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4FGqgkt-XjE"
   },
   "source": [
    "## Gradient descent implementation\n",
    "Here we implement Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TGofDHGV-dRk"
   },
   "outputs": [],
   "source": [
    "alpha = 0.05        # learning parameter\n",
    "maxsteps= 1000      # number of iterations that the algorithm is running\n",
    "\n",
    "# First estimates for our parameters\n",
    "thet = [2, 0]\n",
    "\n",
    "thet, thetaHist = gradientDescent(x, y, thet, alpha, m, maxsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpNMnPCx-sZe"
   },
   "source": [
    "## Print found optimal values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAPuTHlM-urX"
   },
   "outputs": [],
   "source": [
    "print(\"Optimized Theta0 is \", thet[0])\n",
    "print(\"Optimized Theta1 is \", thet[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXmsqwP--v5k"
   },
   "source": [
    "## Visualization of the solution\n",
    "Now let's plot the found solutions of the Gradient Descent algorithms on the contour plot of our cost function to see how it approaches the desired minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 566,
     "status": "error",
     "timestamp": 1594188267487,
     "user": {
      "displayName": "Enrique Hortal Quesada",
      "photoUrl": "",
      "userId": "12024213593623682680"
     },
     "user_tz": -120
    },
    "id": "7HAhC2Kw-0w0",
    "outputId": "d7bef4a7-31cf-4697-d850-a8e8f439859a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c2b4132b8fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthetaHist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthetaHist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'theta 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'theta 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig3 = plt.figure(3)\n",
    "plt.contour(theta0, theta1, np.transpose(J))\n",
    "plt.plot(thetaHist[:,0], thetaHist[:,1], 'x')\n",
    "ax.set_xlabel('theta 0')\n",
    "ax.set_ylabel('theta 1')\n",
    "\n",
    "# Finally, let's plot the hypothesis function into our data\n",
    "xs = np.array([x[0,1], x[x.shape[0]-1,1]])\n",
    "h = np.array([[thet[1] * xs[0] + thet[0]], [thet[1] * xs[1] + thet[0]]])\n",
    "plt.figure(1)\n",
    "plt.plot(x[:,1], y, 'x')  # Data\n",
    "plt.plot(xs, h, '-o')     # hypothesis function\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMj3lnx6l1he2CUNg1xIsQJ",
   "collapsed_sections": [],
   "name": "LinearRegression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
